networks:
  enit_lab2_hive_hadoop_net:
    external: true   # use existing Hadoop network

services:
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    ports:
      - "8079:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - HADOOP_CONF_DIR=hdfs://namenode:9000
    networks:
      - enit_lab2_hive_hadoop_net
    volumes:
      - spark-data:/data  # <-- new volume to share CSVs

  spark-worker-1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-1
    ports:
      - "8081:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - enit_lab2_hive_hadoop_net

  spark-worker-2:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-2
    ports:
      - "8082:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - enit_lab2_hive_hadoop_net
  
  nginx:
    image: nginx:alpine
    container_name: spark-nginx
    ports:
      - "8085:80"   # access web UI on localhost:8085
    volumes:
      - spark-data:/data:ro  # CSVs + HTML/JS go here
      - ./web-ui:/usr/share/nginx/html       # your HTML/JS/CSS
      - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
    networks:
      - enit_lab2_hive_hadoop_net
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - enit_lab2_hive_hadoop_net


  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
     - enit_lab2_hive_hadoop_net

volumes:
  spark-data: