networks:
  enit_lab2_hive_hadoop_net:
    external: true   # use existing Hadoop network

services:
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    ports:
      - "8079:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - HADOOP_CONF_DIR=hdfs://namenode:9000
    networks:
      - enit_lab2_hive_hadoop_net
    volumes:
      - spark-data:/data  # <-- new volume to share CSVs

  spark-worker-1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-1
    ports:
      - "8081:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - enit_lab2_hive_hadoop_net

  spark-worker-2:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-2
    ports:
      - "8082:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - enit_lab2_hive_hadoop_net
  
  nginx:
    image: nginx:alpine
    container_name: spark-nginx
    ports:
      - "8085:80"   # access web UI on localhost:8085
    volumes:
      - spark-data:/usr/share/nginx/html:ro  # CSVs + HTML/JS go here
      - ./web-ui:/usr/share/nginx/html       # your HTML/JS/CSS
    networks:
      - enit_lab2_hive_hadoop_net
volumes:
  spark-data: